# Date: 23 October 2020

## Question:
Let's assume `x` is an independent variable and `y` is the dependent variable. I create two linear models. In model A, I force it pass throgh origin (i.e. no intercept) whereas for model B, there is an intercept. The $R^2$ values for the two model are $0.9729$ and $0.8474$ respectively. Which of the following statement is correct?

(Sample code in R)
``` r
x = runif(20)
y = 2*x + rnorm(20, 0, 0.3)

model_a = lm(y ~ x - 1) #without intercept
summary(model_a)
#> 
#> Call:
#> lm(formula = y ~ x - 1)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -0.33345 -0.22169 -0.01212  0.22156  0.37864 
#> 
#> Coefficients:
#>   Estimate Std. Error t value Pr(>|t|)    
#> x  2.06073    0.07883   26.14 2.34e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.2395 on 19 degrees of freedom
#> Multiple R-squared:  0.9729, Adjusted R-squared:  0.9715 
#> F-statistic: 683.3 on 1 and 19 DF,  p-value: 2.336e-16

model_b = lm(y ~ x) #with intercept
summary(model_b)
#> 
#> Call:
#> lm(formula = y ~ x)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -0.35398 -0.20741 -0.01811  0.22235  0.36850 
#> 
#> Coefficients:
#>             Estimate Std. Error t value Pr(>|t|)    
#> (Intercept) -0.06463    0.14602  -0.443    0.663    
#> x            2.14893    0.21495   9.997 8.97e-09 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.2447 on 18 degrees of freedom
#> Multiple R-squared:  0.8474, Adjusted R-squared:  0.8389 
#> F-statistic: 99.94 on 1 and 18 DF,  p-value: 8.969e-09
```

## Topic:
1. Data Science
2. Linear Regression

## Options:
1. Model A is better than Model B
2. Model B is better than Model A
3. None of the above are correct

## Correct Option:
3. None of the above are correct

## Explanation:
$R^2$ is much larger in the model with no intercept. This does not, however, mean that the relation is “more linear” when the intercept is not included or that more of the variation is explained. What is happening is that the definition of $R^2$ itself is changing. It is most easily seen from the ANOVA tables in the two cases:
``` r
anova(model_a)
#> Analysis of Variance Table
#> 
#> Response: y
#>           Df Sum Sq Mean Sq F value    Pr(>F)    
#> x          1 31.968  31.968  256.21 1.747e-12 ***
#> Residuals 19  2.371   0.125                      
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

anova(model_b)
#> Analysis of Variance Table
#> 
#> Response: y
#>           Df Sum Sq Mean Sq F value    Pr(>F)    
#> x          1 7.5241  7.5241  61.771 3.151e-07 ***
#> Residuals 18 2.1925  0.1218                      
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
Notice that the total sum of squares and the total number of degrees of freedom is not the same in the two analyses. In the model with an intercept, there are 19 DF in all and the total sum of squares is $\sum(y_i - \bar{y})^2$, while the model without an intercept has a total of 20 DF and the total sum of squares is defined as $\sum y_i^2$. Unless $\bar{y}$ is close to zero, the latter “total SS” will be much larger than the former, so if the residual variance is similar, $R^2$ will be much closer to 1.

## Scripts:
1. Question Script: NULL
2. Answer Script: NULL

## Link:
1. Question Link:
   1. "Introductory Statistics with R by Peter Dalagaard"
2. Answer Link:
   1. "Introductory Statistics with R by Peter Dalagaard"

## Images:
1. Question Images:
   1. "../images/questions/q_23102020.png"
2. Answer Images:
   1. "../images/answers/a_23102020.png"
